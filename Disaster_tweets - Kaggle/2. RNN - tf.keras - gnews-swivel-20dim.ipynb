{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaggingClassifier + TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining the 3 columns ( keyword + location + text ) - filling the NAN by a blank or ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['text'] = training_data['keyword'].fillna('') + training_data['location'].fillna('')  \\\n",
    "                        + training_data['text'].fillna('')\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = training_data.drop(columns=['id','keyword','location'],axis=1)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning step involves tokenizing, removing the stopwords and removing non-alphanumeric tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    tokens = text.split()\n",
    "    no_stopwords = [x for x in tokens if x not in stop_words]\n",
    "    no_nonalphanum = [x.lower() for x in no_stopwords if x.isalnum()]\n",
    "    return ' '.join(no_nonalphanum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>our deeds reason may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>all residents asked notified no evacuation she...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>people receive evacuation orders california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>just got sent photo ruby smoke pours school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0              our deeds reason may allah forgive us       1\n",
       "1                   forest fire near la ronge canada       1\n",
       "2  all residents asked notified no evacuation she...       1\n",
       "3        people receive evacuation orders california       1\n",
       "4        just got sent photo ruby smoke pours school       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['text'] = training_data['text'].apply(clean_data)\n",
    "print(training_data.shape)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>just happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>heard different stay safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>forest fire spot geese fleeing across i cannot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>apocalypse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>typhoon soudelor kills 28 china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                   just happened terrible car crash\n",
       "1                          heard different stay safe\n",
       "2  forest fire spot geese fleeing across i cannot...\n",
       "3                                         apocalypse\n",
       "4             typhoon soudelor kills 28 china taiwan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('test.csv')\n",
    "test_id = test_data['id']\n",
    "test_data['text'] = test_data['keyword'].fillna('') + test_data['location'].fillna('')  \\\n",
    "                        + test_data['text'].fillna('')\n",
    "test_data = test_data.drop(columns=['id','keyword','location'],axis=1)\n",
    "test_data['text'] = test_data['text'].apply(clean_data)\n",
    "print(test_data.shape)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download pre-trained word embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating neural network of 4 layers, having a word embeddings layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 20)                400020    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 400,373\n",
      "Trainable params: 400,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(training_data.text,training_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5709 samples\n",
      "Epoch 1/50\n",
      "5709/5709 [==============================] - 2s 317us/sample - loss: 0.6007 - accuracy: 0.6716\n",
      "Epoch 2/50\n",
      "5709/5709 [==============================] - 1s 229us/sample - loss: 0.5126 - accuracy: 0.7518\n",
      "Epoch 3/50\n",
      "5709/5709 [==============================] - 1s 228us/sample - loss: 0.4773 - accuracy: 0.7730\n",
      "Epoch 4/50\n",
      "5709/5709 [==============================] - 1s 226us/sample - loss: 0.4461 - accuracy: 0.7893\n",
      "Epoch 5/50\n",
      "5709/5709 [==============================] - 1s 234us/sample - loss: 0.4159 - accuracy: 0.8075\n",
      "Epoch 6/50\n",
      "5709/5709 [==============================] - 1s 243us/sample - loss: 0.3869 - accuracy: 0.8224\n",
      "Epoch 7/50\n",
      "5709/5709 [==============================] - 1s 228us/sample - loss: 0.3588 - accuracy: 0.8336\n",
      "Epoch 8/50\n",
      "5709/5709 [==============================] - 1s 226us/sample - loss: 0.3315 - accuracy: 0.8520\n",
      "Epoch 9/50\n",
      "5709/5709 [==============================] - 1s 225us/sample - loss: 0.3050 - accuracy: 0.8648\n",
      "Epoch 10/50\n",
      "5709/5709 [==============================] - 1s 229us/sample - loss: 0.2806 - accuracy: 0.8749\n",
      "Epoch 11/50\n",
      "5709/5709 [==============================] - 1s 229us/sample - loss: 0.2558 - accuracy: 0.8847\n",
      "Epoch 12/50\n",
      "5709/5709 [==============================] - 1s 237us/sample - loss: 0.2344 - accuracy: 0.8998\n",
      "Epoch 13/50\n",
      "5709/5709 [==============================] - 1s 225us/sample - loss: 0.2146 - accuracy: 0.9080\n",
      "Epoch 14/50\n",
      "5709/5709 [==============================] - 1s 225us/sample - loss: 0.1956 - accuracy: 0.9154\n",
      "Epoch 15/50\n",
      "5709/5709 [==============================] - 1s 229us/sample - loss: 0.1801 - accuracy: 0.9233\n",
      "Epoch 16/50\n",
      "5709/5709 [==============================] - 1s 237us/sample - loss: 0.1657 - accuracy: 0.9287\n",
      "Epoch 17/50\n",
      "5709/5709 [==============================] - 1s 229us/sample - loss: 0.1534 - accuracy: 0.9357\n",
      "Epoch 18/50\n",
      "5709/5709 [==============================] - 1s 231us/sample - loss: 0.1426 - accuracy: 0.9394\n",
      "Epoch 19/50\n",
      "5709/5709 [==============================] - 1s 226us/sample - loss: 0.1334 - accuracy: 0.9422\n",
      "Epoch 20/50\n",
      "5709/5709 [==============================] - 1s 221us/sample - loss: 0.1247 - accuracy: 0.9490\n",
      "Epoch 21/50\n",
      "5709/5709 [==============================] - 1s 225us/sample - loss: 0.1177 - accuracy: 0.9527\n",
      "Epoch 22/50\n",
      "5709/5709 [==============================] - 1s 224us/sample - loss: 0.1115 - accuracy: 0.9529\n",
      "Epoch 23/50\n",
      "5709/5709 [==============================] - 2s 272us/sample - loss: 0.1066 - accuracy: 0.9567\n",
      "Epoch 24/50\n",
      "5709/5709 [==============================] - 1s 245us/sample - loss: 0.1010 - accuracy: 0.9571\n",
      "Epoch 25/50\n",
      "5709/5709 [==============================] - 1s 249us/sample - loss: 0.0968 - accuracy: 0.9608\n",
      "Epoch 26/50\n",
      "5709/5709 [==============================] - 1s 259us/sample - loss: 0.0932 - accuracy: 0.9606\n",
      "Epoch 27/50\n",
      "5709/5709 [==============================] - 1s 257us/sample - loss: 0.0895 - accuracy: 0.9653\n",
      "Epoch 28/50\n",
      "5709/5709 [==============================] - 2s 269us/sample - loss: 0.0870 - accuracy: 0.9636\n",
      "Epoch 29/50\n",
      "5709/5709 [==============================] - 2s 295us/sample - loss: 0.0845 - accuracy: 0.9664\n",
      "Epoch 30/50\n",
      "5709/5709 [==============================] - 2s 290us/sample - loss: 0.0819 - accuracy: 0.9662\n",
      "Epoch 31/50\n",
      "5709/5709 [==============================] - 2s 355us/sample - loss: 0.0787 - accuracy: 0.9672\n",
      "Epoch 32/50\n",
      "5709/5709 [==============================] - 2s 270us/sample - loss: 0.0768 - accuracy: 0.9683\n",
      "Epoch 33/50\n",
      "5709/5709 [==============================] - 2s 288us/sample - loss: 0.0760 - accuracy: 0.9683\n",
      "Epoch 34/50\n",
      "5709/5709 [==============================] - 1s 255us/sample - loss: 0.0741 - accuracy: 0.9695\n",
      "Epoch 35/50\n",
      "5709/5709 [==============================] - 2s 264us/sample - loss: 0.0723 - accuracy: 0.9695\n",
      "Epoch 36/50\n",
      "5709/5709 [==============================] - 1s 245us/sample - loss: 0.0703 - accuracy: 0.9690\n",
      "Epoch 37/50\n",
      "5709/5709 [==============================] - 1s 252us/sample - loss: 0.0694 - accuracy: 0.9713\n",
      "Epoch 38/50\n",
      "5709/5709 [==============================] - 1s 242us/sample - loss: 0.0683 - accuracy: 0.9714\n",
      "Epoch 39/50\n",
      "5709/5709 [==============================] - 2s 287us/sample - loss: 0.0673 - accuracy: 0.9699\n",
      "Epoch 40/50\n",
      "5709/5709 [==============================] - 2s 270us/sample - loss: 0.0667 - accuracy: 0.9713\n",
      "Epoch 41/50\n",
      "5709/5709 [==============================] - 2s 278us/sample - loss: 0.0659 - accuracy: 0.9716\n",
      "Epoch 42/50\n",
      "5709/5709 [==============================] - 2s 267us/sample - loss: 0.0649 - accuracy: 0.9734\n",
      "Epoch 43/50\n",
      "5709/5709 [==============================] - 2s 280us/sample - loss: 0.0641 - accuracy: 0.9732\n",
      "Epoch 44/50\n",
      "5709/5709 [==============================] - 2s 272us/sample - loss: 0.0634 - accuracy: 0.9723\n",
      "Epoch 45/50\n",
      "5709/5709 [==============================] - 2s 267us/sample - loss: 0.0625 - accuracy: 0.9713\n",
      "Epoch 46/50\n",
      "5709/5709 [==============================] - 2s 276us/sample - loss: 0.0629 - accuracy: 0.9741\n",
      "Epoch 47/50\n",
      "5709/5709 [==============================] - 2s 263us/sample - loss: 0.0615 - accuracy: 0.9734\n",
      "Epoch 48/50\n",
      "5709/5709 [==============================] - 2s 293us/sample - loss: 0.0596 - accuracy: 0.9739\n",
      "Epoch 49/50\n",
      "5709/5709 [==============================] - 2s 318us/sample - loss: 0.0594 - accuracy: 0.9743\n",
      "Epoch 50/50\n",
      "5709/5709 [==============================] - 2s 280us/sample - loss: 0.0590 - accuracy: 0.9744\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1904/1904 - 0s - loss: 1.8421 - accuracy: 0.7206\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test,y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7205882352941176"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(x_test)\n",
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7613 samples\n",
      "Epoch 1/50\n",
      "7613/7613 [==============================] - 2s 270us/sample - loss: 0.4061 - accuracy: 0.9087\n",
      "Epoch 2/50\n",
      "7613/7613 [==============================] - 2s 273us/sample - loss: 0.2491 - accuracy: 0.9211\n",
      "Epoch 3/50\n",
      "7613/7613 [==============================] - 2s 281us/sample - loss: 0.2069 - accuracy: 0.9280\n",
      "Epoch 4/50\n",
      "7613/7613 [==============================] - 2s 278us/sample - loss: 0.1840 - accuracy: 0.9320\n",
      "Epoch 5/50\n",
      "7613/7613 [==============================] - 2s 277us/sample - loss: 0.1673 - accuracy: 0.9379\n",
      "Epoch 6/50\n",
      "7613/7613 [==============================] - 2s 292us/sample - loss: 0.1550 - accuracy: 0.9415\n",
      "Epoch 7/50\n",
      "7613/7613 [==============================] - 2s 293us/sample - loss: 0.1439 - accuracy: 0.9450\n",
      "Epoch 8/50\n",
      "7613/7613 [==============================] - 2s 301us/sample - loss: 0.1349 - accuracy: 0.9471\n",
      "Epoch 9/50\n",
      "7613/7613 [==============================] - 2s 307us/sample - loss: 0.1261 - accuracy: 0.9510\n",
      "Epoch 10/50\n",
      "7613/7613 [==============================] - 2s 296us/sample - loss: 0.1192 - accuracy: 0.9509\n",
      "Epoch 11/50\n",
      "7613/7613 [==============================] - 2s 303us/sample - loss: 0.1126 - accuracy: 0.9528\n",
      "Epoch 12/50\n",
      "7613/7613 [==============================] - 2s 307us/sample - loss: 0.1064 - accuracy: 0.9565\n",
      "Epoch 13/50\n",
      "7613/7613 [==============================] - 2s 327us/sample - loss: 0.1005 - accuracy: 0.9577\n",
      "Epoch 14/50\n",
      "7613/7613 [==============================] - 2s 316us/sample - loss: 0.0969 - accuracy: 0.9590\n",
      "Epoch 15/50\n",
      "7613/7613 [==============================] - 3s 353us/sample - loss: 0.0925 - accuracy: 0.9620\n",
      "Epoch 16/50\n",
      "7613/7613 [==============================] - 3s 356us/sample - loss: 0.0890 - accuracy: 0.9624\n",
      "Epoch 17/50\n",
      "7613/7613 [==============================] - 2s 327us/sample - loss: 0.0861 - accuracy: 0.9644\n",
      "Epoch 18/50\n",
      "7613/7613 [==============================] - 2s 309us/sample - loss: 0.0828 - accuracy: 0.9652\n",
      "Epoch 19/50\n",
      "7613/7613 [==============================] - 2s 320us/sample - loss: 0.0802 - accuracy: 0.9670\n",
      "Epoch 20/50\n",
      "7613/7613 [==============================] - 2s 318us/sample - loss: 0.0781 - accuracy: 0.9668\n",
      "Epoch 21/50\n",
      "7613/7613 [==============================] - 3s 392us/sample - loss: 0.0762 - accuracy: 0.9679\n",
      "Epoch 22/50\n",
      "7613/7613 [==============================] - 3s 399us/sample - loss: 0.0744 - accuracy: 0.9682\n",
      "Epoch 23/50\n",
      "7613/7613 [==============================] - 3s 346us/sample - loss: 0.0729 - accuracy: 0.9690\n",
      "Epoch 24/50\n",
      "7613/7613 [==============================] - 3s 402us/sample - loss: 0.0718 - accuracy: 0.9701\n",
      "Epoch 25/50\n",
      "7613/7613 [==============================] - 3s 381us/sample - loss: 0.0702 - accuracy: 0.9695\n",
      "Epoch 26/50\n",
      "7613/7613 [==============================] - 3s 432us/sample - loss: 0.0695 - accuracy: 0.9704 - loss: 0.0\n",
      "Epoch 27/50\n",
      "7613/7613 [==============================] - 3s 421us/sample - loss: 0.0681 - accuracy: 0.9707\n",
      "Epoch 28/50\n",
      "7613/7613 [==============================] - 3s 415us/sample - loss: 0.0677 - accuracy: 0.9716\n",
      "Epoch 29/50\n",
      "7613/7613 [==============================] - 3s 386us/sample - loss: 0.0667 - accuracy: 0.9699\n",
      "Epoch 30/50\n",
      "7613/7613 [==============================] - 3s 432us/sample - loss: 0.0661 - accuracy: 0.9701\n",
      "Epoch 31/50\n",
      "7613/7613 [==============================] - 4s 479us/sample - loss: 0.0653 - accuracy: 0.9716\n",
      "Epoch 32/50\n",
      "7613/7613 [==============================] - 3s 382us/sample - loss: 0.0646 - accuracy: 0.9711\n",
      "Epoch 33/50\n",
      "7613/7613 [==============================] - 2s 318us/sample - loss: 0.0641 - accuracy: 0.9719\n",
      "Epoch 34/50\n",
      "7613/7613 [==============================] - 3s 355us/sample - loss: 0.0629 - accuracy: 0.9716\n",
      "Epoch 35/50\n",
      "7613/7613 [==============================] - 3s 387us/sample - loss: 0.0621 - accuracy: 0.9720 - loss: 0.0628 - accuracy: \n",
      "Epoch 36/50\n",
      "7613/7613 [==============================] - 3s 363us/sample - loss: 0.0627 - accuracy: 0.9725\n",
      "Epoch 37/50\n",
      "7613/7613 [==============================] - 2s 309us/sample - loss: 0.0619 - accuracy: 0.9719\n",
      "Epoch 38/50\n",
      "7613/7613 [==============================] - 3s 340us/sample - loss: 0.0617 - accuracy: 0.9716\n",
      "Epoch 39/50\n",
      "7613/7613 [==============================] - 2s 322us/sample - loss: 0.0616 - accuracy: 0.9723\n",
      "Epoch 40/50\n",
      "7613/7613 [==============================] - 2s 318us/sample - loss: 0.0606 - accuracy: 0.9739\n",
      "Epoch 41/50\n",
      "7613/7613 [==============================] - 2s 312us/sample - loss: 0.0611 - accuracy: 0.9728\n",
      "Epoch 42/50\n",
      "7613/7613 [==============================] - 2s 292us/sample - loss: 0.0608 - accuracy: 0.9715\n",
      "Epoch 43/50\n",
      "7613/7613 [==============================] - 2s 306us/sample - loss: 0.0602 - accuracy: 0.9722\n",
      "Epoch 44/50\n",
      "7613/7613 [==============================] - 3s 342us/sample - loss: 0.0595 - accuracy: 0.9729\n",
      "Epoch 45/50\n",
      "7613/7613 [==============================] - 2s 319us/sample - loss: 0.0594 - accuracy: 0.9725\n",
      "Epoch 46/50\n",
      "7613/7613 [==============================] - 2s 311us/sample - loss: 0.0599 - accuracy: 0.9725\n",
      "Epoch 47/50\n",
      "7613/7613 [==============================] - 3s 371us/sample - loss: 0.0586 - accuracy: 0.9731\n",
      "Epoch 48/50\n",
      "7613/7613 [==============================] - 2s 304us/sample - loss: 0.0588 - accuracy: 0.9728\n",
      "Epoch 49/50\n",
      "7613/7613 [==============================] - 2s 308us/sample - loss: 0.0586 - accuracy: 0.9724\n",
      "Epoch 50/50\n",
      "7613/7613 [==============================] - 2s 304us/sample - loss: 0.0579 - accuracy: 0.9736\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_data.text,training_data.target,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data['text']\n",
    "predictions = model.predict_classes(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.reshape(-1)\n",
    "results = pd.DataFrame({\n",
    "                        'id':test_id,\n",
    "                        'target':predictions\n",
    "                        })\n",
    "results.to_csv('submissions.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy :---\n",
    "#####                - Local Accuracy : 73.52 (over split training data)\n",
    "#####                - Online Accuracy : 72.59 (After fitting over all training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
